
# üìù –û—Ç—á–µ—Ç –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É –∑–∞–¥–∞–Ω–∏—é ETL (4 –º–æ–¥—É–ª—å)

## üì¶ –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

- **–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö:** `transactions_v2.csv` (—Ä–∞–∑–º–µ—â—ë–Ω –≤ Object Storage Yandex Cloud)
- **ETL-–ø–∞–π–ø–ª–∞–π–Ω:** Airflow ‚Üí Dataproc (PySpark) ‚Üí Kafka ‚Üí PostgreSQL (Managed Service)
- **–§–æ—Ä–º–∞—Ç—ã:** CSV ‚Üí Parquet ‚Üí JSON ‚Üí Kafka message ‚Üí PostgreSQL

---

## üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞

### –ó–∞–¥–∞–Ω–∏–µ 1. üîÑÔ∏è –†–∞–±–æ—Ç–∞ —Å Yandex DataTransfer

- –í—Ä—É—á–Ω—É—é —Å–æ–∑–¥–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –≤ YDB
  <details>
    <summary>–¢—É—Ç SQL —Å–∫—Ä–∏–ø—Ç</summary>
  
    ### sql-—Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ YDB
    ```sql
    CREATE TABLE transactions_v2 (
          msno Utf8,
          payment_method_id Int32,
          payment_plan_days Int32,
          plan_list_price Int32,
          actual_amount_paid Int32,
          is_auto_renew Int8,
          transaction_date Utf8,
          membership_expire_date Utf8,
          is_cancel Int8,
          PRIMARY KEY (msno)
      );
    ```
  </details> 
- –í —Å–æ–∑–¥–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø–æ–º–æ—â—å—é CLI –∑–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç transaction_v2
  <details>
    <summary>–¢—É—Ç bash —Å–∫—Ä–∏–ø—Ç</summary>
  
    ### bash-—Å–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
    ```bash
    ydb  `
    --endpoint grpcs://ydb.serverless.yandexcloud.net:2135 `
    --database /ru-central1/b1g9tm1cvjc9r6hl0g83/etn4ciikjn2811hfpjo9 `
    --sa-key-file authorized_key.json `
    import file csv `
    --path transactions_v2 `
    --delimiter "," `
    --skip-rows 1 `
    --null-value "" `
    --verbose `
    transactions_v2.csv
    ```
  </details> 
- –°–æ–∑–¥–∞–Ω —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –≤ YDB –∏ –ø—Ä–∏–µ–º–Ω–∏–∫–æ–º –≤ Object Storage
  `s3a://etl-data-transform/transactions_v2.parquet`
    	<details>
    	<summary>–¢—É—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç—ã</summary>
		- ![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/screenshot1.jpg)
  		- ![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/screenshot2.jpg)
  		- ![–°–∫—Ä–∏–Ω—à–æ—Ç](screenshots/screenshot3.jpg)
	</details> 

### –ó–∞–¥–∞–Ω–∏–µ 2.  üñ•Ô∏è –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å Yandex Data Processing –ø—Ä–∏ –ø–æ–º–æ—â–∏ Apache AirFlow

- –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (Managed service for Airflow)
- –°–æ–∑–¥–∞–Ω DAG **DATA_INGEST**, –∫–æ—Ç–æ—Ä—ã–π:
    - –°–æ–∑–¥–∞–µ—Ç Data Proc –∫–ª–∞—Å—Ç–µ—Ä.      
    - –ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ PySpark-–∑–∞–¥–∞–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Parquet-—Ñ–∞–π–ª–∞.
    - –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–¥–∞–Ω–∏—è —É–¥–∞–ª—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä.
  	<details>
    		<summary>–¢—É—Ç —Ç–µ–∫—Å—Ç DAG</summary>
  
	 ### Data-proc-DAG.py
  
	 ```python
	    import uuid
		import datetime
		from airflow import DAG
		from airflow.utils.trigger_rule import TriggerRule
		from airflow.providers.yandex.operators.yandexcloud_dataproc import (
		    DataprocCreateClusterOperator,
		    DataprocCreatePysparkJobOperator,
		    DataprocDeleteClusterOperator,
		)
		
		# –î–∞–Ω–Ω—ã–µ –≤–∞—à–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
		YC_DP_AZ = 'ru-central1-a'
		YC_DP_SSH_PUBLIC_KEY = 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIL7QzQcp0xQqFK6vEAo+hrKFwEWDYi9+ypctkf1LxcyE vasil@LES_PC'
		YC_DP_SUBNET_ID = 'e9btfqefvs4ved64rkg6'
		YC_DP_SA_ID = 'ajerrplc7q4nqek3211q'
		YC_DP_METASTORE_URI = '10.128.0.15'
		YC_BUCKET = 'etl-dataproc'
		
		# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ DAG
		with DAG(
		        'DATA_INGEST',
		        schedule_interval='@hourly',
		        tags=['data-processing-and-airflow'],
		        start_date=datetime.datetime.now(),
		        max_active_runs=1,
		        catchup=False
		) as ingest_dag:
		    # 1 —ç—Ç–∞–ø: —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Yandex Data Proc
		    create_spark_cluster = DataprocCreateClusterOperator(
		        task_id='dp-cluster-create-task',
		        cluster_name=f'tmp-dp-{uuid.uuid4()}',
		        cluster_description='–í—Ä–µ–º–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è PySpark-–∑–∞–¥–∞–Ω–∏—è –ø–æ–¥ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–µ–π Managed Service for Apache Airflow‚Ñ¢',
		        ssh_public_keys=YC_DP_SSH_PUBLIC_KEY,
		        service_account_id=YC_DP_SA_ID,
		        subnet_id=YC_DP_SUBNET_ID,
		        s3_bucket=YC_BUCKET,
		        zone=YC_DP_AZ,
		        cluster_image_version='2.1',
		        masternode_resource_preset='s2.small',  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ—Å—É—Ä—Å–Ω—ã–π –ø—Ä–µ—Å–µ—Ç
		        masternode_disk_type='network-hdd',
		        masternode_disk_size=32,  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∏—Å–∫–∞
		        computenode_resource_preset='s2.small',  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–µ—Å—É—Ä—Å–Ω—ã–π –ø—Ä–µ—Å–µ—Ç
		        computenode_disk_type='network-hdd',
		        computenode_disk_size=32,  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∏—Å–∫–∞
		        computenode_count=1,  # —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤
		        computenode_max_hosts_count=3,  # —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
		        services=['YARN', 'SPARK'],
		        datanode_count=0,
		        properties={
		            'spark:spark.hive.metastore.uris': f'thrift://{YC_DP_METASTORE_URI}:9083',
		        },
		    )
	
	    # 2 —ç—Ç–∞–ø: –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞–Ω–∏—è PySpark
	    poke_spark_processing = DataprocCreatePysparkJobOperator(
	        task_id='dp-cluster-pyspark-task',
	        main_python_file_uri=f's3a://{YC_BUCKET}/scripts/clean-data.py',
	    )
	
	    # 3 —ç—Ç–∞–ø: —É–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Yandex Data Processing
	    delete_spark_cluster = DataprocDeleteClusterOperator(
	        task_id='dp-cluster-delete-task',
	        trigger_rule=TriggerRule.ALL_DONE,
	    )
	
	    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ DAG –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤—ã—à–µ —ç—Ç–∞–ø–æ–≤
	    create_spark_cluster >> poke_spark_processing >> delete_spark_cluster```
		
</details> 
- –í–Ω—É—Ç—Ä–∏ —Å–∫—Ä–∏–ø—Ç–∞-–∑–∞–¥–∞–Ω–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∑–∞–≥—Ä—É–∑–∫–∞, –æ—á–∏—Å—Ç–∫–∞ –∏ –∑–∞–ø–∏—Å—å –æ—á–∏—â–µ–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö :
  - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –≤—Å–µ—Ö –ø–æ–ª–µ–π (`Integer`, `Boolean`, `Date`, `String`)
  - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
  - –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ `transaction_date` –∏ `membership_expire_date` –∏–∑ `yyyyMMdd` –≤ `DateType`
- –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ Parquet:
  - `s3a://etl-data-transform/transactions_v2_clean.parquet`
  <details>
    <summary>–¢—É—Ç —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞</summary>
  
	### clean-data.py
		  
	```python
		from pyspark.sql import SparkSession
		from pyspark.sql.functions import col, to_date
		from pyspark.sql.types import IntegerType, StringType, BooleanType
		from pyspark.sql.utils import AnalysisException
		
		
		# === Spark session ===
		spark = SparkSession.builder.appName("Parquet ETL with Logging to S3").getOrCreate()
		
		
		# === –ü—É—Ç–∏ ===
		source_path = "s3a://etl-data-source/transactions_v2.csv"
		target_path = "s3a://etl-data-transform/transactions_v2_clean.parquet"
		
		try:
		    print(f"–ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {source_path}")
		    df = spark.read.option("header", "true").option("inferSchema", "true").csv(source_path)
		
		    print("–°—Ö–µ–º–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
		    df.printSchema()
		
		    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ + —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã YYYYMMDD
		    df = df.withColumn("actual_amount_paid", col("actual_amount_paid").cast(IntegerType())) \
		           .withColumn("is_auto_renew", col("is_auto_renew").cast(BooleanType())) \
		           .withColumn("is_cancel", col("is_cancel").cast(BooleanType())) \
		           .withColumn("membership_expire_date", to_date(col("membership_expire_date").cast("string"), "yyyyMMdd")) \
		           .withColumn("msno", col("msno").cast(StringType())) \
		           .withColumn("payment_method_id", col("payment_method_id").cast(IntegerType())) \
		           .withColumn("payment_plan_days", col("payment_plan_days").cast(IntegerType())) \
		           .withColumn("plan_list_price", col("plan_list_price").cast(IntegerType())) \
		           .withColumn("transaction_date", to_date(col("transaction_date").cast("string"),  "yyyyMMdd"))
		
		    print("–°—Ö–µ–º–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
		    df.printSchema()
		
		    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
		    df = df.na.drop()
		
		    print("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:")
		    df.show(5)
		
		    print(f"–ó–∞–ø–∏—Å—å –≤ Parquet: {target_path}")
		    df.write.mode("overwrite").parquet(target_path)
		
		    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Parquet.")

		except AnalysisException as ae:
		    print("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞:", ae)
		except Exception as e:
		    print("‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞:", e)
	
		spark.stop()
	```
  	</details> 
### –ó–∞–¥–∞–Ω–∏–µ 3. üì§ –†–∞–±–æ—Ç–∞ —Å —Ç–æ–ø–∏–∫–∞–º–∏ Apache Kafka¬Æ —Å –ø–æ–º–æ—â—å—é PySpark-–∑–∞–¥–∞–Ω–∏–π –≤ Yandex Data Processing

- –°–æ–∑–¥–∞–Ω –∫–ª–∞—Å—Ç–µ—Ä Data Proc, –ø–æ–¥–Ω—è—Ç Managed service for Kafka
- –í Object Storage –ø–æ–º–µ—â–µ–Ω—ã —Å–∫—Ä–∏–ø—Ç—ã:  
	- –°–∫—Ä–∏–ø—Ç `kafka-write.py`:
	  - –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Parquet-—Ñ–∞–π–ª–∞ —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
	  - –ö–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É –≤—ã–±–∏—Ä–∞–µ—Ç 100 —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫.
	  - –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ JSON.
	  - –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ Kafka-—Ç–æ–ø–∏–∫ `dataproc-kafka-topic`.
     		<details>
    		<summary>–¢—É—Ç —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞</summary>
  
		### kafka-write.py
		  
		```python
		import time
		from pyspark.sql import SparkSession
		from pyspark.sql.functions import col, to_json, struct, rand
		
		def main():
		    spark = SparkSession.builder \
			.appName("parquet-to-kafka-loop-json") \
			.getOrCreate()

	    # –ß—Ç–µ–Ω–∏–µ parquet-—Ñ–∞–π–ª–∞
	    df = spark.read.parquet("s3a://etl-data-transform/transactions_v2_clean.parquet").cache()
	    total = df.count()
	    print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {total} —Å—Ç—Ä–æ–∫")
	
	    while True:
		# 100 —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫
		batch_df = df.orderBy(rand()).limit(100)
	
		# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ JSON
		kafka_df = batch_df.select(to_json(struct([col(c) for c in batch_df.columns])).alias("value"))
	
		# –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Kafka
		kafka_df.write \
		    .format("kafka") \
		    .option("kafka.bootstrap.servers", "rc1a-sp0t812fps48sn74.mdb.yandexcloud.net:9091") \
		    .option("topic", "dataproc-kafka-topic") \
		    .option("kafka.security.protocol", "SASL_SSL") \
		    .option("kafka.sasl.mechanism", "SCRAM-SHA-512") \
		    .option("kafka.sasl.jaas.config",
			    "org.apache.kafka.common.security.scram.ScramLoginModule required "
			    "username=\"user1\" "
			    "password=\"password1\";") \
		    .save()

	        print("‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ 100 —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –≤ Kafka")
	        time.sleep(1)
	
	    	spark.stop()
	
		if __name__ == "__main__":
		    main()
	    	```
	</details>
   
	- Kafka –∏—Å–ø–æ–ª—å–∑—É–µ—Ç:
	  - –ü—Ä–æ—Ç–æ–∫–æ–ª: `SASL_SSL`
	  - –ú–µ—Ö–∞–Ω–∏–∑–º: `SCRAM-SHA-512`

### 3. üì• –ß—Ç–µ–Ω–∏–µ –∏–∑ Kafka –∏ –∑–∞–ø–∏—Å—å –≤ PostgreSQL

- –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ PySpark:
  - –ß–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka.
  - –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ—Ç JSON.
  - –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç—ã.
  - –ü–∏—à–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü—É PostgreSQL `transactions_stream` —Å –ø–æ–º–æ—â—å—é `.foreachBatch()`.

---

## üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Yandex DataLens

**–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:**
- üîç –§–∏–ª—å—Ç—Ä –ø–æ –ø–æ–ª—é `msno`
- üìÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Å–Ω–∞—á–∞–ª–∞ –Ω–æ–≤—ã–µ)

### üîπ –î–∞—à–±–æ—Ä–¥—ã –∏ —á–∞—Ä—Ç—ã

| –ù–∞–∑–≤–∞–Ω–∏–µ —á–∞—Ä—Ça             | –ò—Å—Ç–æ—á–Ω–∏–∫        | –ê–≤—Ç–æ—Ä          | –î–∞—Ç–∞     | –û–ø–∏—Å–∞–Ω–∏–µ                                          |
|----------------------------|------------------|----------------|----------|---------------------------------------------------|
| üí≥ –ú–µ—Ç–æ–¥—ã –æ–ø–ª–∞—Ç—ã –ø–æ —Å—É–º–º–µ  | kafka-stream     | skyranger2008  | 15.06.25 | –¢–æ–ø-10 payment_method_id –ø–æ `actual_amount_paid` |
| üìÜ –°—É–º–º–∞ –ø–æ –º–µ—Å—è—Ü–∞–º         | kafka-stream     | skyranger2008  | 15.06.25 | –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ `transaction_date`, —Å—É–º–º–∞         |
| üìÜ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ –º–µ—Å—è—Ü–∞–º    | kafka-stream     | skyranger2008  | 15.06.25 | –ö–æ–ª-–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –ø–æ –º–µ—Å—è—Ü–∞–º                      |
| üßæ –ú–µ—Ç–æ–¥—ã –æ–ø–ª–∞—Ç—ã            | kafka-stream     | skyranger2008  | 15.06.25 | –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ `payment_method_id`             |

---

## ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

- ‚úÖ –§–∞–π–ª CSV —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, –æ—á–∏—â–µ–Ω, —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Parquet
- ‚úÖ Kafka-—Ç–æ–ø–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
- ‚úÖ PostgreSQL —Ç–∞–±–ª–∏—Ü–∞ –Ω–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥ –∏–∑ Kafka
- ‚úÖ –î–∞–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤ Yandex DataLens
